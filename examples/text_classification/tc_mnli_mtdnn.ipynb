{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*\n",
    "\n",
    "*Licensed under the MIT License.*\n",
    "\n",
    "# The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MT-DNN is an open-source natural language understanding (NLU) toolkit that makes it easy for researchers and developers to train customized deep learning models. Built upon PyTorch and Transformers, MT-DNN is designed to facilitate rapid\n",
    "customization for a broad spectrum of NLU tasks, using a variety of objectives (classification, regression, structured prediction) and text encoders (e.g., RNNs, BERT, RoBERTa, UniLM). A unique feature of MT-DNN is its built-in support for robust and transferable learning using the adversarial multi-task learning paradigm. To enable efficient production deployment, MT-DNN supports multitask knowledge distillation, which can substantially compress a deep neural model without significant performance drop. We demonstrate the effectiveness of MT-DNN on a wide range of NLU applications across general and biomedical domains. The pip installable package and pretrained models will be publicly available at https://github.com/microsoft/mt-dnn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MT-DNN is designed for modularity, flexibility, and ease of use. These modules are built upon PyTorch (Paszke et al., 2019) and Transformers (Wolf\n",
    "et al., 2019), allowing the use of the SOTA pretrained models, e.g., BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019c) and UniLM (Dong\n",
    "et al., 2019). The unique attribute of this package is a flexible interface for adversarial multi-task fine-tuning and knowledge distillation, so that researchers and developers can build large SOTA NLU models and then compress them to small ones\n",
    "for online deployment.The overall workflow and system architecture are shown in figures 1 and 3 respectively.\n",
    "\n",
    "\n",
    "![Workflow Design](https://nlpbp.blob.core.windows.net/images/mt-dnn2.JPG)\n",
    "\n",
    "The above figure shows workflow of MT-DNN: train a neural language model on a large amount of unlabeled raw text\n",
    "to obtain general contextual representations; then finetune the learned contextual representation on downstream tasks, e.g. GLUE (Wang et al., 2018); lastly, distill this large model to a lighter one for online deployment. In the later two phrases, we can leverage powerful multi-task learning and adversarial training to further improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![overall_arch](https://nlpbp.blob.core.windows.net/images/mt-dnn.png)\n",
    "The figure above shows the overall system architecture. The lower layers are shared across all tasks while the top layers are taskspecific. The input X (either a sentence or a set of sentences) is first represented as a sequence of embedding\n",
    "vectors, one for each word, in l1. Then the encoder, e.g a Transformer or recurrent neural network (LSTM) model,\n",
    "captures the contextual information for each word and generates the shared contextual embedding vectors in l2.\n",
    "Finally, for each task, additional task-specific layers generate task-specific representations, followed by operations\n",
    "necessary for classification, similarity scoring, or relevance ranking. In case of adversarial training, we perturb\n",
    "embeddings from the lexicon encoder and then add an extra loss term during the training. Note that for the\n",
    "inference phrase, it does not require perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Text Classification of MultiNLI Sentences using MT-DNN\n",
    "\n",
    "This notebook utilizes the pip installable package that implements the Multi-Task Deep Neural Network Toolkit (MTDNN) for Natural Language Understanding. It's recommended to run this notebook on GPU machines as it's very computationally intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/useradmin/miniconda3/envs/nlp_gpu/lib/python3.6/site-packages/dask/dataframe/utils.py:15: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "import torch\n",
    "import shutil\n",
    "\n",
    "from tempfile import TemporaryDirectory\n",
    "from utils_nlp.dataset.multinli import download_tsv_files_and_extract\n",
    "\n",
    "from mtdnn.common.types import EncoderModelType\n",
    "from mtdnn.configuration_mtdnn import MTDNNConfig\n",
    "from mtdnn.modeling_mtdnn import MTDNNModel\n",
    "from mtdnn.process_mtdnn import MTDNNDataProcess\n",
    "from mtdnn.tasks.config import MTDNNTaskDefs\n",
    "from mtdnn.data_builder_mtdnn import MTDNNDataBuilder\n",
    "from mtdnn.tokenizer_mtdnn import MTDNNTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Configuration, Tasks and Model Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we fine-tune and evaluate MT-DNN models on a subset of the [MultiNLI](https://www.nyu.edu/projects/bowman/multinli/) dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Configuration, Tasks and Model Objects\n",
    "ROOT_DIR = TemporaryDirectory().name\n",
    "OUTPUT_DIR = os.path.join(ROOT_DIR, 'checkpoint')\n",
    "os.makedirs(OUTPUT_DIR) if not os.path.exists(OUTPUT_DIR) else OUTPUT_DIR\n",
    "\n",
    "LOG_DIR = os.path.join(ROOT_DIR, 'tensorboard_logdir')\n",
    "os.makedirs(LOG_DIR) if not os.path.exists(LOG_DIR) else LOG_DIR\n",
    "\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "os.makedirs(DATA_DIR) if not os.path.exists(DATA_DIR) else DATA_DIR\n",
    "\n",
    "DATA_SOURCE_DIR = os.path.join(DATA_DIR, \"MNLI\")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the location for our data to be downloaded, model to be checkpointed and logs to be dumped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpv27d9tk4/data\n",
      "/tmp/tmpv27d9tk4/checkpoint\n",
      "/tmp/tmpv27d9tk4/tensorboard_logdir\n"
     ]
    }
   ],
   "source": [
    "print(DATA_DIR)\n",
    "print(OUTPUT_DIR)\n",
    "print(LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset\n",
    "We start by loading a subset of the data. The following function also downloads and extracts the files, if they don't exist in the data folder.\n",
    "\n",
    "The MultiNLI dataset is mainly used for natural language inference (NLI) tasks, where the inputs are sentence pairs and the labels are entailment indicators. The sentence pairs are also classified into *genres* that allow for more coverage and better evaluation of NLI models.\n",
    "\n",
    "For our classification task, we use the first sentence only as the text input, and the corresponding genre as the label. We select the examples corresponding to one of the entailment labels (*neutral* in this case) to avoid duplicate rows, as the sentences are not unique, whereas the sentence pairs are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305k/305k [00:04<00:00, 62.6kKB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded file to:  /tmp/tmpv27d9tk4/data/MNLI\n"
     ]
    }
   ],
   "source": [
    "download_tsv_files_and_extract(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Configuration Object \n",
    "\n",
    "Create a model configuration object, `MTDNNConfig`, with the necessary parameters to initialize the MT-DNN model. Initialization without any parameters will default to a similar configuration that initializes a BERT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MTDNNConfig(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create Task Definition Object  \n",
    "\n",
    "Define the task parameters to train for and initialize an `MTDNNTaskDefs` object. Create a task parameter dictionary. Definition can be a single or multiple tasks to train.  `MTDNNTaskDefs` can take a python dict, yaml or json file with task(s) defintion.\n",
    "\n",
    "The data source directory is the path of data downloaded and extracted above using `download_tsv_files_and_extract` which is the `MNLI` dir under the `DATA_DIR` temporary directory.    \n",
    "\n",
    "The data source has options that are set to drive each task pre-processing; `data_process_opts`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/24/2020 11:56:20 - mtdnn.tasks.config - INFO - Mapping Task attributes\n",
      "06/24/2020 11:56:20 - mtdnn.tasks.config - INFO - Configured task definitions - ['mnli']\n"
     ]
    }
   ],
   "source": [
    "tasks_params = {\n",
    "    \"mnli\": {\n",
    "        \"data_format\": \"PremiseAndOneHypothesis\",\n",
    "        \"encoder_type\": \"BERT\",\n",
    "        \"dropout_p\": 0.3,\n",
    "        \"enable_san\": True,\n",
    "        \"labels\": [\"contradiction\", \"neutral\", \"entailment\"],\n",
    "        \"metric_meta\": [\"ACC\"],\n",
    "        \"loss\": \"CeCriterion\",\n",
    "        \"kd_loss\": \"MseCriterion\",\n",
    "        \"n_class\": 3,\n",
    "        \"split_names\": [\n",
    "            \"train\",\n",
    "            \"dev_matched\",\n",
    "            \"dev_mismatched\",\n",
    "            \"test_matched\",\n",
    "            \"test_mismatched\",\n",
    "        ],\n",
    "        \"data_source_dir\": DATA_SOURCE_DIR,\n",
    "        \"data_process_opts\": {\"header\": True, \"is_train\": True, \"multi_snli\": False,},\n",
    "        \"task_type\": \"Classification\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define the tasks\n",
    "task_defs = MTDNNTaskDefs(tasks_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create the MTDNN Data Tokenizer Object  \n",
    "\n",
    "Create a data tokenizing object, `MTDNNTokenizer`. Based on the model initial checkpoint, it wraps around the model's Huggingface transformers library to encode the data to MT-DNN format. This becomes the input to the data building stage.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MTDNNTokenizer(do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing out the Tokenizer encode function on a sample text\n",
    "`tokenizer.encode(\"What NLP toolkit do you recommend\", \"MT-DNN is a fantastic toolkit\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([101, 2054, 17953, 2361, 6994, 23615, 2079, 2017, 16755, 102, 11047, 1011, 1040, 10695, 2003, 1037, 10392, 6994, 23615, 102], None, [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"What NLP toolkit do you recommend\", \"MT-DNN is a fantastic toolkit\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Data Builder Object  \n",
    "\n",
    "Create a data preprocessing object, `MTDNNDataBuilder`. This class is responsible for converting the data into the MT-DNN format depending on the task.  \n",
    " \n",
    "\n",
    "Define a data builder that handles the creating of each task's vectorized data utilizing the model tokenizer. This will build out the vectorized data needed for creating the training, test and development PyTorch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/24/2020 11:58:03 - mtdnn.data_builder_mtdnn - INFO - Sucessfully loaded and built 392702 samples for mnli at /tmp/tmpv27d9tk4/data/canonical_data_2/mnli_train.tsv\n",
      "06/24/2020 11:58:03 - mtdnn.data_builder_mtdnn - INFO - Sucessfully loaded and built 9815 samples for mnli at /tmp/tmpv27d9tk4/data/canonical_data_2/mnli_dev_matched.tsv\n",
      "06/24/2020 11:58:03 - mtdnn.data_builder_mtdnn - INFO - Sucessfully loaded and built 9832 samples for mnli at /tmp/tmpv27d9tk4/data/canonical_data_2/mnli_dev_mismatched.tsv\n",
      "06/24/2020 11:58:03 - mtdnn.data_builder_mtdnn - INFO - Sucessfully loaded and built 9796 samples for mnli at /tmp/tmpv27d9tk4/data/canonical_data_2/mnli_test_matched.tsv\n",
      "06/24/2020 11:58:03 - mtdnn.data_builder_mtdnn - INFO - Sucessfully loaded and built 9847 samples for mnli at /tmp/tmpv27d9tk4/data/canonical_data_2/mnli_test_mismatched.tsv\n",
      "mnli_train\n",
      "06/24/2020 11:58:03 - mtdnn.data_builder_mtdnn - INFO - Building Data For 'MNLI TRAIN' Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Data For Premise and One Hypothesis: 392702it [05:21, 1222.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/25/2020 12:03:26 - mtdnn.data_builder_mtdnn - INFO - Saving data to /tmp/tmpv27d9tk4/data/canonical_data_2/bert_base_uncased/mnli_train.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Data For PremiseAndOneHypothesis: 100%|██████████| 392702/392702 [00:05<00:00, 74643.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnli_dev_matched\n",
      "06/25/2020 12:03:32 - mtdnn.data_builder_mtdnn - INFO - Building Data For 'MNLI DEV MATCHED' Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Data For Premise and One Hypothesis: 9815it [00:07, 1239.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/25/2020 12:03:40 - mtdnn.data_builder_mtdnn - INFO - Saving data to /tmp/tmpv27d9tk4/data/canonical_data_2/bert_base_uncased/mnli_dev_matched.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Data For PremiseAndOneHypothesis: 100%|██████████| 9815/9815 [00:00<00:00, 66228.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnli_dev_mismatched\n",
      "06/25/2020 12:03:40 - mtdnn.data_builder_mtdnn - INFO - Building Data For 'MNLI DEV MISMATCHED' Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Data For Premise and One Hypothesis: 9832it [00:08, 1192.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/25/2020 12:03:48 - mtdnn.data_builder_mtdnn - INFO - Saving data to /tmp/tmpv27d9tk4/data/canonical_data_2/bert_base_uncased/mnli_dev_mismatched.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Data For PremiseAndOneHypothesis: 100%|██████████| 9832/9832 [00:00<00:00, 72484.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnli_test_matched\n",
      "06/25/2020 12:03:48 - mtdnn.data_builder_mtdnn - INFO - Building Data For 'MNLI TEST MATCHED' Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Data For Premise and One Hypothesis: 9796it [00:07, 1256.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/25/2020 12:03:56 - mtdnn.data_builder_mtdnn - INFO - Saving data to /tmp/tmpv27d9tk4/data/canonical_data_2/bert_base_uncased/mnli_test_matched.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Data For PremiseAndOneHypothesis: 100%|██████████| 9796/9796 [00:00<00:00, 67821.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnli_test_mismatched\n",
      "06/25/2020 12:03:56 - mtdnn.data_builder_mtdnn - INFO - Building Data For 'MNLI TEST MISMATCHED' Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Data For Premise and One Hypothesis: 9847it [00:09, 1068.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/25/2020 12:04:05 - mtdnn.data_builder_mtdnn - INFO - Saving data to /tmp/tmpv27d9tk4/data/canonical_data_2/bert_base_uncased/mnli_test_mismatched.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Data For PremiseAndOneHypothesis: 100%|██████████| 9847/9847 [00:00<00:00, 75038.45it/s]\n"
     ]
    }
   ],
   "source": [
    "## Load and build data\n",
    "data_builder = MTDNNDataBuilder(\n",
    "    tokenizer=tokenizer,\n",
    "    task_defs=task_defs,\n",
    "    data_dir=DATA_DIR,\n",
    "    canonical_data_suffix=\"canonical_data\",\n",
    "    dump_rows=True,\n",
    ")\n",
    "\n",
    "## Build data to MTDNN Format\n",
    "## Iterable of each specific task and processed data\n",
    "vectorized_data = data_builder.vectorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Data Processing Object  \n",
    "\n",
    "Create a data preprocessing object, `MTDNNDataProcess`. This creates the training, test and development PyTorch dataloaders needed for training and testing. We also need to retrieve the necessary training options required to initialize the model correctly, for all tasks.  \n",
    "\n",
    "Define a data process that handles creating the training, test and development PyTorch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/25/2020 12:08:47 - mtdnn.process_mtdnn - INFO - Starting to process the training data sets\n",
      "06/25/2020 12:08:47 - mtdnn.process_mtdnn - INFO - Loading mnli_train as task 0\n",
      "06/25/2020 12:08:47 - mtdnn.dataset_mtdnn - INFO - Loaded 392702 samples out of 392702\n",
      "06/25/2020 12:08:47 - mtdnn.process_mtdnn - INFO - Starting to process the testing data sets\n",
      "06/25/2020 12:08:47 - mtdnn.process_mtdnn - INFO - Loading mnli_dev_matched as task 0\n",
      "06/25/2020 12:08:47 - mtdnn.dataset_mtdnn - INFO - Loaded 9815 samples out of 9815\n",
      "06/25/2020 12:08:47 - mtdnn.process_mtdnn - INFO - Loading mnli_dev_mismatched as task 0\n",
      "06/25/2020 12:08:47 - mtdnn.dataset_mtdnn - INFO - Loaded 9832 samples out of 9832\n",
      "06/25/2020 12:08:47 - mtdnn.process_mtdnn - INFO - Loading mnli_test_matched as task 0\n",
      "06/25/2020 12:08:47 - mtdnn.dataset_mtdnn - INFO - Loaded 9796 samples out of 9796\n",
      "06/25/2020 12:08:47 - mtdnn.process_mtdnn - INFO - Loading mnli_test_mismatched as task 0\n",
      "06/25/2020 12:08:47 - mtdnn.dataset_mtdnn - INFO - Loaded 9847 samples out of 9847\n"
     ]
    }
   ],
   "source": [
    "# Make the Data Preprocess step and update the config with training data updates\n",
    "data_processor = MTDNNDataProcess(\n",
    "    config=config, task_defs=task_defs, vectorized_data=vectorized_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the processed batch multitask batch data loaders for training, development and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitask_train_dataloader = data_processor.get_train_dataloader()\n",
    "dev_dataloaders_list = data_processor.get_dev_dataloaders()\n",
    "test_dataloaders_list = data_processor.get_test_dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can retrieve the training options, from the processor, to initialize model with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_opts = data_processor.get_decoder_options_list()\n",
    "task_types = data_processor.get_task_types_list()\n",
    "dropout_list = data_processor.get_tasks_dropout_prob_list()\n",
    "loss_types = data_processor.get_loss_types_list()\n",
    "kd_loss_types = data_processor.get_kd_loss_types_list()\n",
    "tasks_nclass_list = data_processor.get_task_nclass_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us update the batch steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_all_batches = data_processor.get_num_all_batches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the MTDNN Model\n",
    "\n",
    "Now we can go ahead and create an `MTDNNModel` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 0, number of task labels: 3\n"
     ]
    }
   ],
   "source": [
    "model = MTDNNModel(\n",
    "    config,\n",
    "    task_defs,\n",
    "    pretrained_model_name=\"bert-base-uncased\",\n",
    "    num_train_step=num_all_batches,\n",
    "    decoder_opts=decoder_opts,\n",
    "    task_types=task_types,\n",
    "    dropout_list=dropout_list,\n",
    "    loss_types=loss_types,\n",
    "    kd_loss_types=kd_loss_types,\n",
    "    tasks_nclass_list=tasks_nclass_list,\n",
    "    multitask_train_dataloader=multitask_train_dataloader,\n",
    "    dev_dataloaders_list=dev_dataloaders_list,\n",
    "    test_dataloaders_list=test_dataloaders_list,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    log_dir=LOG_DIR \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Finetuning, Prediction and Evaluation\n",
    "\n",
    "### Fit and finetune model on five epochs and predict using the training and test  \n",
    "\n",
    "At this point the MT-DNN model allows us to fit to the model and create predictions. The fit takes an optional `epochs` parameter that overwrites the epochs set in the `MTDNNConfig` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f8e690e3940>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "Perform inference using the last (best) checkpointed model. With 5 epochs, the last model would be `model_4.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(trained_model_chckpt=f\"{OUTPUT_DIR}/model_4.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}\n",
    "dev_result_files = list(filter(lambda x: x.endswith('.json') and 'dev' in x, os.listdir(OUTPUT_DIR))) \n",
    "for d in dev_result_files: \n",
    "    name =  ' '.join(list(map(str.capitalize, d.split('_')))[:3]) \n",
    "    with open(d, 'r') as f: \n",
    "        res = json.load(f) \n",
    "        results.update(\n",
    "            {name: {\n",
    "                'accuracy': res['metrics']['ACC']\n",
    "                }\n",
    "            }) \n",
    "df_results = pd.DataFrame(results)   \n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up temporary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(ROOT_DIR):\n",
    "    shutil.rmtree(ROOT_DIR, ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_gpu)",
   "language": "python",
   "name": "nlp_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
