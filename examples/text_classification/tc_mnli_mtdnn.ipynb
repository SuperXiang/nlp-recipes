{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*\n",
    "\n",
    "*Licensed under the MIT License.*\n",
    "\n",
    "# Text Classification of MultiNLI Sentences using MT-DNN\n",
    "\n",
    "This notebook utilizes a PyTorch package that implements the Multi-Task Deep Neural Network Toolkit (MTDNN) for Natural Language Understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/useradmin/miniconda3/envs/nlp_gpu/lib/python3.6/site-packages/dask/dataframe/utils.py:15: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from tempfile import TemporaryDirectory\n",
    "from utils_nlp.dataset.multinli import download_tsv_files_and_extract\n",
    "\n",
    "from mtdnn.common.types import EncoderModelType\n",
    "from mtdnn.configuration_mtdnn import MTDNNConfig\n",
    "from mtdnn.modeling_mtdnn import MTDNNModel\n",
    "from mtdnn.process_mtdnn import MTDNNDataProcess\n",
    "from mtdnn.tasks.config import MTDNNTaskDefs\n",
    "from mtdnn.data_builder_mtdnn import MTDNNDataBuilder\n",
    "from mtdnn.tokenizer_mtdnn import MTDNNTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Configuration, Tasks and Model Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we fine-tune and evaluate MT-DNN models on a subset of the [MultiNLI](https://www.nyu.edu/projects/bowman/multinli/) dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Configuration, Tasks and Model Objects\n",
    "ROOT_DIR = TemporaryDirectory().name\n",
    "OUTPUT_DIR = os.path.join(ROOT_DIR, 'checkpoint')\n",
    "os.makedirs(OUTPUT_DIR) if not os.path.exists(OUTPUT_DIR) else OUTPUT_DIR\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "os.makedirs(DATA_DIR) if not os.path.exists(DATA_DIR) else DATA_DIR\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 1\n",
    "TRAIN_DATA_FRACTION = 0.05\n",
    "TEST_DATA_FRACTION = 0.05\n",
    "TRAIN_SIZE = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(DATA_DIR)\n",
    "print(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset\n",
    "We start by loading a subset of the data. The following function also downloads and extracts the files, if they don't exist in the data folder.\n",
    "\n",
    "The MultiNLI dataset is mainly used for natural language inference (NLI) tasks, where the inputs are sentence pairs and the labels are entailment indicators. The sentence pairs are also classified into *genres* that allow for more coverage and better evaluation of NLI models.\n",
    "\n",
    "For our classification task, we use the first sentence only as the text input, and the corresponding genre as the label. We select the examples corresponding to one of the entailment labels (*neutral* in this case) to avoid duplicate rows, as the sentences are not unique, whereas the sentence pairs are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305k/305k [00:07<00:00, 43.5kKB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded file to:  /tmp/tmp4rgav0t_/data/MNLI\n"
     ]
    }
   ],
   "source": [
    "download_tsv_files_and_extract(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Configuration Object \n",
    "\n",
    "Create a model configuration object, `MTDNNConfig`, with the necessary parameters to initialize the MT-DNN model. Initialization without any parameters will default to a similar configuration that initializes a BERT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MTDNNConfig(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create Task Definition Object  \n",
    "\n",
    "Define the task parameters to train for and initialize an `MTDNNTaskDefs` object. Create a task parameter dictionary. Definition can be a single or multiple tasks to train.  `MTDNNTaskDefs` can take a python dict, yaml or json file with task(s) defintion.\n",
    "\n",
    "The data source directory is the path of data downloaded and extracted above using `download_tsv_files_and_extract` which is the `MNLI` dir under the `DATA_DIR` temporary directory.    \n",
    "\n",
    "The data source has options that are set to drive each task pre-processing; `data_process_opts`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/19/2020 01:37:36 - mtdnn.tasks.config - INFO - Mapping Task attributes\n",
      "06/19/2020 01:37:36 - mtdnn.tasks.config - INFO - Configured task definitions - ['mnli']\n"
     ]
    }
   ],
   "source": [
    "data_source_dir = os.path.join(DATA_DIR, \"MNLI\")\n",
    "tasks_params = {\n",
    "    \"mnli\": {\n",
    "        \"data_format\": \"PremiseAndOneHypothesis\",\n",
    "        \"encoder_type\": \"BERT\",\n",
    "        \"dropout_p\": 0.3,\n",
    "        \"enable_san\": True,\n",
    "        \"labels\": [\"contradiction\", \"neutral\", \"entailment\"],\n",
    "        \"metric_meta\": [\"ACC\"],\n",
    "        \"loss\": \"CeCriterion\",\n",
    "        \"kd_loss\": \"MseCriterion\",\n",
    "        \"n_class\": 3,\n",
    "        \"split_names\": [\n",
    "            \"train\",\n",
    "            \"dev_matched\",\n",
    "            \"dev_mismatched\",\n",
    "            \"test_matched\",\n",
    "            \"test_mismatched\",\n",
    "        ],\n",
    "        \"data_source_dir\": data_source_dir,\n",
    "        \"data_process_opts\": {\"header\": True, \"is_train\": True, \"multi_snli\": False,},\n",
    "        \"task_type\": \"Classification\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define the tasks\n",
    "task_defs = MTDNNTaskDefs(tasks_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create the MTDNN Data Tokenizer Object  \n",
    "\n",
    "Create a data tokenizing object, `MTDNNTokenizer`. Based on the model initial checkpoint, it wraps around the model's Huggingface transformers library to encode the data to MT-DNN format. This becomes the input to the data building stage.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MTDNNTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing out the Tokenizer encode function on a sample text\n",
    "`tokenizer.encode(\"He is a boy\", \"what is he\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([101, 100, 2003, 1037, 2879, 102, 2054, 2003, 2002, 102], None, [0, 0, 0, 0, 0, 0, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"He is a boy\", \"what is he\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Data Builder Object  \n",
    "\n",
    "Create a data preprocessing object, `MTDNNDataBuilder`. This class is responsible for converting the data into the MT-DNN format depending on the task.  \n",
    " \n",
    "\n",
    "Define a data builder that handles the creating of each task's vectorized data utilizing the model tokenizer. This will build out the vectorized data needed for creating the training, test and development PyTorch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/19/2020 01:37:39 - mtdnn.data_builder_mtdnn - INFO - Sucessfully loaded and built 392702 samples for mnli at /home/useradmin/sources/mt-dnn-orig/data/canonical_data_2/mnli_train.tsv\n",
      "06/19/2020 01:37:39 - mtdnn.data_builder_mtdnn - INFO - Sucessfully loaded and built 9815 samples for mnli at /home/useradmin/sources/mt-dnn-orig/data/canonical_data_2/mnli_dev_matched.tsv\n",
      "06/19/2020 01:37:39 - mtdnn.data_builder_mtdnn - INFO - Sucessfully loaded and built 9832 samples for mnli at /home/useradmin/sources/mt-dnn-orig/data/canonical_data_2/mnli_dev_mismatched.tsv\n",
      "06/19/2020 01:37:39 - mtdnn.data_builder_mtdnn - INFO - Sucessfully loaded and built 9796 samples for mnli at /home/useradmin/sources/mt-dnn-orig/data/canonical_data_2/mnli_test_matched.tsv\n",
      "06/19/2020 01:37:39 - mtdnn.data_builder_mtdnn - INFO - Sucessfully loaded and built 9847 samples for mnli at /home/useradmin/sources/mt-dnn-orig/data/canonical_data_2/mnli_test_mismatched.tsv\n",
      "mnli_train\n",
      "06/19/2020 01:37:39 - mtdnn.data_builder_mtdnn - INFO - Building Data For 'MNLI TRAIN' Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Data For Premise and One Hypothesis: 392702it [03:57, 1654.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/19/2020 01:41:38 - mtdnn.data_builder_mtdnn - INFO - Saving data to /home/useradmin/sources/mt-dnn-orig/data/canonical_data_2/bert_base_uncased/mnli_train.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Data For PremiseAndOneHypothesis: 100%|██████████| 392702/392702 [00:05<00:00, 67824.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnli_dev_matched\n",
      "06/19/2020 01:41:47 - mtdnn.data_builder_mtdnn - INFO - Building Data For 'MNLI DEV MATCHED' Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Data For Premise and One Hypothesis: 9815it [00:06, 1608.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/19/2020 01:41:53 - mtdnn.data_builder_mtdnn - INFO - Saving data to /home/useradmin/sources/mt-dnn-orig/data/canonical_data_2/bert_base_uncased/mnli_dev_matched.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Data For PremiseAndOneHypothesis: 100%|██████████| 9815/9815 [00:00<00:00, 67040.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnli_dev_mismatched\n",
      "06/19/2020 01:41:53 - mtdnn.data_builder_mtdnn - INFO - Building Data For 'MNLI DEV MISMATCHED' Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Data For Premise and One Hypothesis: 9832it [00:06, 1625.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/19/2020 01:41:59 - mtdnn.data_builder_mtdnn - INFO - Saving data to /home/useradmin/sources/mt-dnn-orig/data/canonical_data_2/bert_base_uncased/mnli_dev_mismatched.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Data For PremiseAndOneHypothesis: 100%|██████████| 9832/9832 [00:00<00:00, 71501.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnli_test_matched\n",
      "06/19/2020 01:41:59 - mtdnn.data_builder_mtdnn - INFO - Building Data For 'MNLI TEST MATCHED' Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Data For Premise and One Hypothesis: 9796it [00:05, 1693.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/19/2020 01:42:05 - mtdnn.data_builder_mtdnn - INFO - Saving data to /home/useradmin/sources/mt-dnn-orig/data/canonical_data_2/bert_base_uncased/mnli_test_matched.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Data For PremiseAndOneHypothesis: 100%|██████████| 9796/9796 [00:00<00:00, 75186.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnli_test_mismatched\n",
      "06/19/2020 01:42:05 - mtdnn.data_builder_mtdnn - INFO - Building Data For 'MNLI TEST MISMATCHED' Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Data For Premise and One Hypothesis: 9847it [00:06, 1619.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/19/2020 01:42:11 - mtdnn.data_builder_mtdnn - INFO - Saving data to /home/useradmin/sources/mt-dnn-orig/data/canonical_data_2/bert_base_uncased/mnli_test_mismatched.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Data For PremiseAndOneHypothesis: 100%|██████████| 9847/9847 [00:00<00:00, 67734.49it/s]\n"
     ]
    }
   ],
   "source": [
    "## Load and build data\n",
    "data_builder = MTDNNDataBuilder(\n",
    "    tokenizer=tokenizer,\n",
    "    task_defs=task_defs,\n",
    "    data_dir=\"/home/useradmin/sources/mt-dnn-orig/data\",\n",
    "    canonical_data_suffix=\"canonical_data_2\",\n",
    "    dump_rows=True,\n",
    ")\n",
    "\n",
    "## Build data to MTDNN Format\n",
    "## Iterable of each specific task and processed data\n",
    "vectorized_data = data_builder.vectorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Data Processing Object  \n",
    "\n",
    "Create a data preprocessing object, `MTDNNDataProcess`. This creates the training, test and development PyTorch dataloaders needed for training and testing. We also need to retrieve the necessary training options required to initialize the model correctly, for all tasks.  \n",
    "\n",
    "Define a data process that handles creating the training, test and development PyTorch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/19/2020 01:42:11 - mtdnn.process_mtdnn - INFO - Starting to process the training data sets\n",
      "06/19/2020 01:42:11 - mtdnn.process_mtdnn - INFO - Loading mnli_train as task 0\n",
      "06/19/2020 01:42:12 - mtdnn.dataset_mtdnn - INFO - Loaded 392702 samples out of 392702\n",
      "06/19/2020 01:42:12 - mtdnn.process_mtdnn - INFO - Starting to process the testing data sets\n",
      "06/19/2020 01:42:12 - mtdnn.process_mtdnn - INFO - Loading mnli_dev_matched as task 0\n",
      "06/19/2020 01:42:12 - mtdnn.dataset_mtdnn - INFO - Loaded 9815 samples out of 9815\n",
      "06/19/2020 01:42:12 - mtdnn.process_mtdnn - INFO - Loading mnli_dev_mismatched as task 0\n",
      "06/19/2020 01:42:12 - mtdnn.dataset_mtdnn - INFO - Loaded 9832 samples out of 9832\n",
      "06/19/2020 01:42:12 - mtdnn.process_mtdnn - INFO - Loading mnli_test_matched as task 0\n",
      "06/19/2020 01:42:12 - mtdnn.dataset_mtdnn - INFO - Loaded 9796 samples out of 9796\n",
      "06/19/2020 01:42:12 - mtdnn.process_mtdnn - INFO - Loading mnli_test_mismatched as task 0\n",
      "06/19/2020 01:42:12 - mtdnn.dataset_mtdnn - INFO - Loaded 9847 samples out of 9847\n"
     ]
    }
   ],
   "source": [
    "# Make the Data Preprocess step and update the config with training data updates\n",
    "data_processor = MTDNNDataProcess(\n",
    "    config=config, task_defs=task_defs, vectorized_data=vectorized_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the processed batch multitask batch data loaders for training, development and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitask_train_dataloader = data_processor.get_train_dataloader()\n",
    "dev_dataloaders_list = data_processor.get_dev_dataloaders()\n",
    "test_dataloaders_list = data_processor.get_test_dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can retrieve the training options, from the processor, to initialize model with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_opts = data_processor.get_decoder_options_list()\n",
    "task_types = data_processor.get_task_types_list()\n",
    "dropout_list = data_processor.get_tasks_dropout_prob_list()\n",
    "loss_types = data_processor.get_loss_types_list()\n",
    "kd_loss_types = data_processor.get_kd_loss_types_list()\n",
    "tasks_nclass_list = data_processor.get_task_nclass_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us update the batch steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_all_batches = data_processor.get_num_all_batches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the MTDNN Model\n",
    "\n",
    "Now we can go ahead and create an `MTDNNModel` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b179d447db0b4fcbbec1d397804bb9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "idx: 0, number of task labels: 3\n"
     ]
    }
   ],
   "source": [
    "model = MTDNNModel(\n",
    "    config,\n",
    "    task_defs,\n",
    "    pretrained_model_name=\"bert-base-uncased\",\n",
    "    num_train_step=num_all_batches,\n",
    "    decoder_opts=decoder_opts,\n",
    "    task_types=task_types,\n",
    "    dropout_list=dropout_list,\n",
    "    loss_types=loss_types,\n",
    "    kd_loss_types=kd_loss_types,\n",
    "    tasks_nclass_list=tasks_nclass_list,\n",
    "    multitask_train_dataloader=multitask_train_dataloader,\n",
    "    dev_dataloaders_list=dev_dataloaders_list,\n",
    "    test_dataloaders_list=test_dataloaders_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit on one epoch and predict using the training and test  \n",
    "\n",
    "At this point the MT-DNN model allows us to fit to the model and create predictions. The fit takes an optional `epochs` parameter that overwrites the epochs set in the `MTDNNConfig` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mtdnn.dataset_mtdnn.MTDNNMultiTaskDataset at 0x7f425aa13a58>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multitask_train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_gpu)",
   "language": "python",
   "name": "nlp_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
